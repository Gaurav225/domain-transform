%\section{Your Proposed Method}\label{sec:method}
\section{Optimizing Domain Transform}

This section describes the baseline implementation of the algorithm and the optimization steps we executed that made a significant impact on the performance of the code. Finally the section also describes the optimisation approaches we tried to use, which did not result in a significant performance improvement, and discuss why it might be the case for our algorithm.

\subsection{Baseline version}

Our baseline implementation is a straightforward port to C++ of the matlab code provided on the homepage\footnote{\url{http://www.inf.ufrgs.br/~eslgastal/DomainTransform/}} of the authors of the paper describing the algorithm~\cite{GastalOliveira2011DomainTransform}.

\fixme{?}
In order to achieve the biggest optimisation potential the matrices have been wrapped in a simple struct (listing \ref{code:matrix_struct}).

\begin{lstlisting}[caption=Matrix struct,label=code:matrix_struct]
struct Mat2
{
    uint width, height;
    float3* data; // size: width*height
};
\end{lstlisting}

In order to preserve the precision of calculations all images are store in a \lstinline{float3} struct, which uses $3\times 4 = 12$ bytes per pixel. Hence a $1$M pixel image is stored in $3\times 4\times 10^6$\ B $=12$\ MB.

The algorithm starts with some initialisation procedures. During initialisation two matrices - $dIdx$ and $dIdy$ - are precomputed, which contain the differentials of the neighbouring image pixels in both $x$ and $y$ directions. Afterwards a prefix sum is computed for both $dIdx$ and $dIdy^T$, creating matrices which in fact contain for each pixel a mapping into 1D coordinate space along each direction.

During the filtering step (see listing~\ref{code:filterstep}) we compute \lstinline{computeBound} and \lstinline{boxFilter} twice. After each computation step we transpose the images.

\begin{lstlisting}[caption=Filterstep,label=code:filterstep]
for (i=0; i<iterations; i++) {
    bounds = computeBound(img, dIdX, r);
    img = boxFilter(img, bounds);
    img = transpose(img);
    bounds = computeBound(img, dIdY, r);
    img = boxFilter(img, dIdY, bounds);
    img = transpose(img);
}
\end{lstlisting}

The \lstinline{computeBound} method computes the 1D bound for each pixel for the box filter, which is dependent on the predefined filter radius. 

Box filter specific initialisation is executed before the actual box filter when the prefix sum of the pixel values is precomputed for further use. This cannot be done once for the whole algorithm and needs to be recomputed, as the pixel values change during execution. Then the \lstinline{boxFilter} makes use of the previously computed prefix sum and filter bounds to compute the filtered value for each pixel (listing \ref{code:boxfilter}).

\begin{lstlisting}[caption=Boxfilter step, label=code:boxfilter]
void boxfilter(img, bounds) {
    psum = prefixsum(img);
    for (i = 0; i < H; i++) {
        for (j = 0; j < W; j++) {
            l = lowerBound(i,j);
            r = upperBound(i,j);
            delta = r - l;
            img(i,j) = psum(l)-psum(r);
            img(i,j) = img(i,j)/delta;
        }
    }
}
\end{lstlisting}

\subsection{Optimization Steps}

\subsubsection{Blocked Transpose}

After benchmarking the initial version we recognised that the transpose function was responsible for most of our computation time. Hence we blocked the transpose function (listing \ref{code:blocked_transpose}).

\begin{lstlisting}[caption=Transpose block, label=code:blocked_transpose]
for (yo=0; yo<Hmod; yo+=BLOCK)
  for (xo=0; xo<Wmod; xo+=BLOCK)
    for (uint y=yo; y<yo+BLOCK; y++)
      for (uint x=xo; x<xo+BLOCK; x++)
        uint idx = y*W + x;
        uint idxT = x*H + y;
        out.data[idxT] = in.data[idx];
    xo = Wmod;
    for (uint y=yo; y<yo+BLOCK; y++)
      for (uint x=xo; x<W; x++)
        uint idx = y*W + x;
        uint idxT = x*H + y;
        out.data[idxT] = in.data[idx];
// Handle bottom row and bottom rightmost block
\end{lstlisting}

\subsubsection{Inlining Bound Computation}

In order to allow the compiler to optimise the code we inlined the bound computation, which also allowed us to spare an additional pass through the data.

\begin{lstlisting}[caption=Inlining of the bound computation, label=code:inlining]
void boxfilter(img, dx, boxR) {
    psum = prefixsum(img);
    for (i=0; i<H; i++)
    {
        posL = 0;
        posR = 0;
        for (j=0; j<W; j++)
        {
            // Bound computation
            dtL = dx(i,j) - boxR;
            dtR = dx(i,j) + boxR;
            while (dx(i,posL) < dtL 
                    && posL < W-1)
                posL++;
            while (posR < W && 
                   dx(i,posR < dtR )
                posR++;
            // Boxfilter step
        }
    }
\end{lstlisting}

\subsubsection{Recombination}

In this optimisation step we tried to combine as many computation as we could in as few passes through the data as possible. We rearranged the initialisation step to conform to that idea, and inlined the prefix sum computation into the \lstinline{boxFilter} bound computation step, getting a sliding window approach (listing~\ref{code:recombination}).

\begin{lstlisting}[caption=Recombination, label=code:recombination]
    while (dx(i,posL) < dtL 
            && posL < W-1)
        sum -= imgRow[posL];
        posL++;

    while (posR < W && 
           dx(i,posR < dtR )
        sum  += imgRow[posR];
        posR++;
    // Boxfilter step
    invD = 1.0f /(posR-posL);
    img(i,j) = sum * invD;
\end{lstlisting}

\subsubsection{Write transpose}

Previously we have transposed the data after each pass of the filter to keep the locality of the computation. However this requires going through the whole image, and by directly storing the data transposed we were able to increase performance and reduce the number of explicit transpositions needed to be computed. 

\fixme{finish}

We note that 

\subsection{Failed optimization attempts}

\fixme{Rewrite}
\subsubsection{Precomputation} 

For each pixel we compute the filter bounds on the fly, which then determines the size of the box filter window. We then compute the average of all pixels in this window, which includes a divison by the number of pixels in the window. This window size can vary a lot between pixels, however it is bounded by the larger of the two image dimensions.

For this reason we precomputed a table of the inverses of all possible division factors and replaced the costly division by a much cheaper multiplication operation.

This did not give a performance increase which might be due to several factors. Firstly it might be that such precomputed values already exist, as these rather small factors which might be used a lot. Or, secondly, it is possible that there was some increase in performance, but this was unnoticeable because the additional array accesses caused more cache misses, and, consequently, more overhead.

\subsubsection{Changing image storage datatype}

\fixme{Roofline plots}

We tried to store the per pixel color information in three \texttt{unsigned char} variables instead of the same number of \texttt{float} fields. This reduced the precision of intermediate storage but also reduces memory requirements for image storage by 75\%. 

This introduced additional cast operations after each load and before each store which might be one of the reasons why this didn't give any improved performance. Besides this, we are most likely not memory bound at the moment, an observation that is confirmed by our roofline plots.

\comment{
Apart from changing the precision that would also mean conversion overhead between float and char, as we still need to average over the pixels, which requires floating point operations.
}

\subsubsection{Blocking}

Unfortunately, one of the most common optimization approaches does not work for the algorithm in question. In general blocking allows to increase cache locality, especially when the block sizes are calculated in a way such that the working set fits in cache. However that also means that the computation for each image pixel should depend on a fixed number of adjacent pixels, which isn't the case for the edge-aware filter.

We have gathered statistics on the amount of pixels used for the test images, and these were numbers between $30$ and $150$ pixels. That would mean that our block sizes should be around $150$ pixels, which is too much to be a reasonable size. Also this number may increase up to the whole dimension of the image as the filter bound depend on the image data (for example in the case of a monochrome line in the image with a wide filter window). Smaller block sizes also don't make sense as in that case we would need to re-read information if the filter bound goes outside the block which would mean more overhead. 

\fixme{Do we have an explanation for that?}
We also tried blocking the computation for spatial locality in the output matrix, as the faster versions write the transposed output directly. For some reason this only made the computation run even slower that the non-blocked version.

\subsubsection{Instruction interleaving}

We tried to combine the while loops that compute the borders for the filter, so that independent computation would fill the pipeline. This did not give any performance improvement, perhaps because of the fact that the combined loop still had conditional statements in between the addition operations we wanted to pipeline.

\subsubsection{Loop unrolling and reordering}

We write the filtered image data in a transposed manner with the consequence that these writes are not memory aligned. Thus, for every pixel written, a new cache line is accessed and invalidated till the next access to it. This causes an additional memory overhead as always the whole chache line will be loaded from memory.

To improve on this situation we unrolled and reordered the loop iterations, with the goal that the computations happen in a more blocked fashion, causing more memory aligned writing operations. However this slowed down our code, possibly due to the increased amount of temporary register variables necessary.

We also tried to tackle this problem by using stream-write intrinsics (direct writes that bypass the cache), but again with no success.

\subsubsection{Reshuffling of data}

\fixme{Rewrite}

Most of the versions we implemented work on the interleaved colour arrays, storing colours as $rgbrgb\dots$. We attempted to reshuffle the data to three separate arrays, each containing one colour, which didn't bring any performance improvement. 

This behaviour could be due to the fact that that way we now need to transpose three separate arrays still, one for each colour. We might write them transposed as well, however then we would access 3 different arrays at a large stride, none of which would get reused in any way as by next access these addresses will be already replaced.

%\\
%\fixme{Write.}
%\comment{
%Now comes the ``beef'' of the paper, where you explain what you
%did. Again, organize it in paragraphs with titles. As in every section
%you start with a very brief overview of the section.
%
%For this class, explain all the optimizations you performed. This mean, you first very briefly
%explain the baseline implementation, then go through locality and other optimizations, and finally SSE (every project will be slightly different of course). 
%Show or mention relevant analysis or assumptions. A few examples: 
%1) Profiling may lead you to optimize one part first; 
%2) bandwidth plus data transfer analysis may show that it is memory bound; 
%3) it may be too hard to implement the algorithm in full generality: make assumptions and state them 
%(e.g., we assume $n$ is divisible by 4; or, we consider only one type of input image); 
%4) explain how certain data accesses have poor locality. Generally, any type of analysis adds value to your work.
%
%As important as the final results is to show that you took a structured, organized approach to the optimization and that you explain why you did what you did.
%
%Mention and cite any external resources including library or other code.
%
%Good visuals or even brief code snippets to illustrate what you did are good. Pasting large amounts of code to fill the space is not good.
%}